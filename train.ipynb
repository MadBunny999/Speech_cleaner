{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''seed: 42\ndataset:\n  shift: 4000\n  sample_len: 16384\n  sample_rate: 16000\n  train:\n    ann_path: 'dataset/train.txt'\n    dataloader:\n      batch_size: 32\n      num_workers: 10\n  val:\n    ann_path: 'dataset/val.txt'\n    dataloader:\n      batch_size: 32\n      num_workers: 10\n  test:\n    ann_path: 'dataset/test.txt'\n    dataloader:\n      batch_size: 1\n      num_workers: 10\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport librosa.display\n\ndef make_plot_spectrogram(stftaudio_magnitude_db,sample_rate, hop_length_fft) :\n    \"\"\"This function plots a spectrogram\"\"\"\n    plt.figure(figsize=(12, 6))\n    librosa.display.specshow(stftaudio_magnitude_db, x_axis='time', y_axis='linear',\n                             sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    title = 'hop_length={},  time_steps={},  fft_bins={}  (2D resulting shape: {})'\n    plt.title(title.format(hop_length_fft,\n                           stftaudio_magnitude_db.shape[1],\n                           stftaudio_magnitude_db.shape[0],\n                           stftaudio_magnitude_db.shape));\n    return\n\ndef make_plot_phase(stft_phase,sample_rate,hop_length_fft) :\n    \"\"\"This function plots the phase in radian\"\"\"\n    plt.figure(figsize=(12, 6))\n    librosa.display.specshow(np.angle(stft_phase), x_axis='time', y_axis='linear',\n                             sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    title = 'hop_length={},  time_steps={},  fft_bins={}  (2D resulting shape: {})'\n    plt.title(title.format(hop_length_fft,\n                           stft_phase.shape[1],\n                           stft_phase.shape[0],\n                           stft_phase.shape));\n    return\n\ndef make_plot_time_serie(audio,sample_rate):\n    \"\"\"This function plots the audio as a time serie\"\"\"\n    plt.figure(figsize=(12, 6))\n    #plt.ylim(-0.05, 0.05)\n    plt.title('Audio')\n    plt.ylabel('Amplitude')\n    plt.xlabel('Time(s)')\n    librosa.display.waveplot(audio, sr=sample_rate)\n    return\n\n\ndef make_3plots_spec_voice_noise(stftvoicenoise_mag_db,stftnoise_mag_db,stftvoice_mag_db,sample_rate, hop_length_fft):\n    \"\"\"This function plots the spectrograms of noisy voice, noise and voice as a single plot \"\"\"\n    plt.figure(figsize=(8, 12))\n    plt.subplot(3, 1, 1)\n    plt.title('Spectrogram voice + noise')\n    librosa.display.specshow(stftvoicenoise_mag_db, x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    plt.subplot(3, 1, 2)\n    plt.title('Spectrogram predicted voice')\n    librosa.display.specshow(stftnoise_mag_db, x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    plt.subplot(3, 1, 3)\n    plt.title('Spectrogram true voice')\n    librosa.display.specshow(stftvoice_mag_db, x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    plt.tight_layout()\n\n    return\n\n\ndef make_3plots_phase_voice_noise(stftvoicenoise_phase,stftnoise_phase,stftvoice_phase,sample_rate, hop_length_fft):\n    \"\"\"This function plots the phase in radians of noisy voice, noise and voice as a single plot \"\"\"\n    plt.figure(figsize=(8, 12))\n    plt.subplot(3, 1, 1)\n    plt.title('Phase (radian) voice + noise')\n    librosa.display.specshow(np.angle(stftvoicenoise_phase), x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    plt.subplot(3, 1, 2)\n    plt.title('Phase (radian) predicted voice')\n    librosa.display.specshow(np.angle(stftnoise_phase), x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    plt.subplot(3, 1, 3)\n    plt.title('Phase (radian) true voice')\n    librosa.display.specshow(np.angle(stftvoice_phase), x_axis='time', y_axis='linear',sr=sample_rate, hop_length=hop_length_fft)\n    plt.colorbar()\n    plt.tight_layout()\n\n    return\n\n\ndef make_3plots_timeseries_voice_noise(clipvoicenoise,clipnoise,clipvoice, sample_rate) :\n    \"\"\"This function plots the time series of audio of noisy voice, noise and voice as a single plot \"\"\"\n    #y_ax_min = min(clipvoicenoise) - 0.15\n    #y_ax_max = max(clipvoicenoise) + 0.15\n\n    plt.figure(figsize=(18, 12))\n    plt.subplots_adjust(hspace=0.35)\n    plt.subplot(3, 1, 1)\n    plt.title('Audio voice + noise')\n    plt.ylabel('Amplitude')\n    plt.xlabel('Time(s)')\n    librosa.display.waveplot(clipvoicenoise, sr=sample_rate)\n    plt.ylim(-0.05, 0.05)\n    plt.subplot(3, 1, 2)\n    plt.title('Audio predicted voice')\n    plt.ylabel('Amplitude')\n    plt.xlabel('Time(s)')\n    librosa.display.waveplot(clipnoise, sr=sample_rate)\n    plt.ylim(-0.05, 0.05)\n    plt.subplot(3, 1, 3)\n    plt.title('Audio true voice')\n    plt.ylabel('Amplitude')\n    plt.xlabel('Time(s)')\n    librosa.display.waveplot(clipvoice, sr=sample_rate)\n    plt.ylim(-0.05, 0.05)\n\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\npaths = sorted(Path(\"../input/ms-snsd/train/mix\").glob('*'))\ntrain_noisy_paths = []\nstr(paths[2])[10:]\nfor i in paths[:1000]:\n    train_noisy_paths.append(str(i))\n\npaths1 = sorted(Path(\"../input/ms-snsd/dev/mix\").glob('*'))\ntast_noisy_paths = []\nstr(paths1[2])[10:]\nfor i in paths1:\n    tast_noisy_paths.append(str(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_noisy_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tast_noisy_paths[7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string =  '../input/ms-snsd/train/mix/noisy1180_SNRdb_5.0_clnsp1180.wav'\nstring.split('_')[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_paths = []\nfor i in train_noisy_paths:\n    clean_paths.append(i[:23]+'clean/'+ i.split('_')[-1])\nclean_paths[9]\n \ndef get_clean_file(path):\n    return path[:23]+'clean/'+ path.split('_')[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport os, pickle\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport librosa\nfrom pydub import AudioSegment\nimport IPython\nimport scipy\n\n\ndef inverse_stft_transform(stft_features, window_length, overlap):\n    return librosa.istft(stft_features, win_length=window_length, hop_length=overlap)\n\n\ndef play(audio, sample_rate):\n    # ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file\n    IPython.display.Audio(data=audio, rate=sample_rate)\n#     sd.play(audio, sample_rate, blocking=True)\n\n\ndef read_audio(filepath, sample_rate, normalize=True):\n    audio, sr = librosa.load(filepath, sr=sample_rate)\n    if normalize is True:\n        div_fac = 1 / np.max(np.abs(audio)) / 3.0\n        audio = audio * div_fac\n        # audio = librosa.util.normalize(audio)\n    return audio, sr\n\nclass FeatureExtractor:\n    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n        self.audio = audio\n        self.ffT_length = windowLength\n        self.window_length = windowLength\n        self.overlap = overlap\n        self.sample_rate = sample_rate\n        self.window = scipy.signal.hamming(self.window_length, sym=False)\n\n    def get_stft_spectrogram(self):\n        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n                            window=self.window, center=True)\n\n    def get_audio_from_stft_spectrogram(self, stft_features):\n        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n                             window=self.window, center=True)\n\n    def get_mel_spectrogram(self):\n        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n                                              n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n\n    def get_audio_from_mel_spectrogram(self, M):\n        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length,\n                                                    hop_length=self.overlap,\n                                                    win_length=self.window_length, window=self.window,\n                                                    center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio, _ = read_audio(clean_paths[9], 16000)\naudio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(file, fname):\n\n    wav = librosa.load(file, sr=16000)[0]\n    stft = librosa.stft(y=wav, n_fft=hparams['n_fft_den'], hop_length=hparams['hop_size_den'], win_length=hparams['win_size_den'])\n    print(\"STFT: \", stft.shape)\n\n    # Display magnitude spectrogram\n    D = np.abs(stft)\n    librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),y_axis='log', x_axis='time')\n    plt.title('Power spectrogram')\n    plt.colorbar(format='%+2.0f dB')\n    plt.tight_layout()\n    plt.show()\n    plt.savefig(fname+\".jpg\")\n    plt.clf()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(clean_paths[9], \"Аудио № 9\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Audio(data=audio, rate=16000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"superAudio = FeatureExtractor(audio, windowLength=256, overlap=round(256/4), sample_rate=16000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"superAudio.get_mel_spectrogram","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hparams = dict(\n    num_mels=80,  # Number of mel-spectrogram channels and local conditioning dimensionality\n    #  network\n    rescale=True,  # Whether to rescale audio prior to preprocessing\n    rescaling_max=0.9,  # Rescaling value\n\n    # For cases of OOM (Not really recommended, only use if facing unsolvable OOM errors, \n    # also consider clipping your samples to smaller chunks)\n    max_mel_frames=900,\n    # Only relevant when clip_mels_length = True, please only use after trying output_per_steps=3\n    #  and still getting OOM errors.\n    \n    # Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction\n    # It\"s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder\n    # Does not work if n_ffit is not multiple of hop_size!!\n    use_lws=False,\n    \n    n_fft=800,  # Extra window size is filled with 0 paddings to match this parameter\n    hop_size=200,  # For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)\n    win_size=800,  # For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\n    sample_rate=16000,  # 16000Hz (corresponding to librispeech) (sox --i <filename>)\n\n    n_fft_den=512,\n    hop_size_den=160,\n    win_size_den=400,\n    \n    frame_shift_ms=None,  # Can replace hop_size parameter. (Recommended: 12.5)\n    \n    # Mel and Linear spectrograms normalization/scaling and clipping\n    signal_normalization=False,\n    # Whether to normalize mel spectrograms to some predefined range (following below parameters)\n    allow_clipping_in_normalization=False,  # Only relevant if mel_normalization = True\n    symmetric_mels=False,\n    # Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2, \n    # faster and cleaner convergence)\n    max_abs_value=4.,\n    # max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not \n    # be too big to avoid gradient explosion, \n    # not too small for fast convergence)\n    normalize_for_wavenet=False,\n    # whether to rescale to [0, 1] for wavenet. (better audio quality)\n    clip_for_wavenet=False,\n    # whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)\n    \n    # Contribution by @begeekmyfriend\n    # Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude \n    # levels. Also allows for better G&L phase reconstruction)\n    preemphasize=False,  # whether to apply filter\n    preemphasis=0.97,  # filter coefficient.\n    \n    # Limits\n    min_level_db=-100,\n    ref_level_db=20,\n    fmin=55,\n    # Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To \n    # test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\n    fmax=7600,  # To be increased/reduced depending on data.\n    \n    # Griffin Lim\n    power=1.5,\n    # Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.\n    griffin_lim_iters=60,\n    # Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.\n    ###########################################################################################################################################\n    \n    N=25,\n    img_size=96,\n    fps=25,\n        \n    n_gpu=1,\n    batch_size=16,\n    num_workers=32,\n    initial_learning_rate=1e-3,\n    reduced_learning_rate=None,\n    nepochs=200,\n    ckpt_freq=1,\n    validation_interval=3,\n\n    wav_step_size=16000,\n    mel_step_size=16,\n    spec_step_size=100,\n    wav_step_overlap=3200\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport librosa.filters\nimport numpy as np\nfrom scipy import signal\n\ndef load_wav(path, sr):\n    return librosa.core.load(path, sr=sr)[0]\n\ndef preemphasis(wav, k, preemphasize=True):\n    if preemphasize:\n        return signal.lfilter([1, -k], [1], wav)\n    return wav\n\ndef inv_preemphasis(wav, k, inv_preemphasize=True):\n    if inv_preemphasize:\n        return signal.lfilter([1], [1, -k], wav)\n    return wav\n\ndef get_hop_size(hparams):\n    hop_size = hparams['hop_size']\n    if hop_size is None:\n        assert hparams['frame_shift_ms'] is not None\n        hop_size = int(hparams['frame_shift_ms'] / 1000 * hparams['sample_rate'])\n    return hop_size\n\ndef linearspectrogram(wav, hparams):\n    D = _stft(preemphasis(wav, hparams['preemphasis'], hparams['preemphasize']), hparams)\n    S = _amp_to_db(np.abs(D), hparams) - hparams['ref_level_db']\n    \n    if hparams['signal_normalization']:\n        return _normalize(S, hparams)\n    return S\n\ndef melspectrogram(wav, hparams):\n    D = _stft(preemphasis(wav, hparams['preemphasis'], hparams['preemphasize']), hparams)\n    S = _amp_to_db(_linear_to_mel(np.abs(D), hparams), hparams) - hparams['ref_level_db']\n    \n    if hparams['signal_normalization']:\n        return _normalize(S, hparams)\n    return S\n\ndef inv_linear_spectrogram(linear_spectrogram, hparams):\n    \"\"\"Converts linear spectrogram to waveform using librosa\"\"\"\n    if hparams.signal_normalization:\n        D = _denormalize(linear_spectrogram, hparams)\n    else:\n        D = linear_spectrogram\n    \n    S = _db_to_amp(D + hparams['ref_level_db']) #Convert back to linear\n    \n    if hparams.use_lws:\n        processor = _lws_processor(hparams)\n        D = processor.run_lws(S.astype(np.float64).T ** hparams['power'])\n        y = processor.istft(D).astype(np.float32)\n        return inv_preemphasis(y, hparams['preemphasis'], hparams['preemphasize'])\n    else:\n        return inv_preemphasis(_griffin_lim(S ** hparams['power'], hparams), hparams['preemphasis'], hparams['preemphasize'])\n\ndef inv_mel_spectrogram(mel_spectrogram, hparams):\n    \"\"\"Converts mel spectrogram to waveform using librosa\"\"\"\n    if hparams.signal_normalization:\n        D = _denormalize(mel_spectrogram, hparams)\n    else:\n        D = mel_spectrogram\n    \n    S = _mel_to_linear(_db_to_amp(D + hparams['ref_level_db']), hparams)  # Convert back to linear\n    \n    if hparams.use_lws:\n        processor = _lws_processor(hparams)\n        D = processor.run_lws(S.astype(np.float64).T ** hparams['power'])\n        y = processor.istft(D).astype(np.float32)\n        return inv_preemphasis(y, hparams['preemphasis'], hparams['preemphasize'])\n    else:\n        return inv_preemphasis(_griffin_lim(S ** hparams['power'], hparams), hparams['preemphasis'], hparams['preemphasize'])\n\ndef _lws_processor(hparams):\n    import lws\n    return lws.lws(hparams['n_fft'], get_hop_size(hparams), fftsize=hparams['win_size'], mode=\"speech\")\n\ndef _griffin_lim(S, hparams):\n    \"\"\"librosa implementation of Griffin-Lim\n    Based on https://github.com/librosa/librosa/issues/434\n    \"\"\"\n    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n    S_complex = np.abs(S).astype(np.complex)\n    y = _istft(S_complex * angles, hparams)\n    for i in range(hparams['griffin_lim_iters']):\n        angles = np.exp(1j * np.angle(_stft(y, hparams)))\n        y = _istft(S_complex * angles, hparams)\n    return y\n\ndef _stft(y, hparams):\n    if hparams['use_lws']:\n        return _lws_processor(hparams).stft(y).T\n    else:\n        return librosa.stft(y=y, n_fft=hparams['n_fft'], hop_length=get_hop_size(hparams), win_length=hparams['win_size'])\n\ndef _istft(y, hparams):\n    return librosa.istft(y, hop_length=get_hop_size(hparams), win_length=hparams['win_size'])\n\n# Conversions\n_mel_basis = None\n_inv_mel_basis = None\n\ndef _linear_to_mel(spectogram, hparams):\n    global _mel_basis\n    if _mel_basis is None:\n        _mel_basis = _build_mel_basis(hparams)\n    return np.dot(_mel_basis, spectogram)\n\ndef _mel_to_linear(mel_spectrogram, hparams):\n    global _inv_mel_basis\n    if _inv_mel_basis is None:\n        _inv_mel_basis = np.linalg.pinv(_build_mel_basis(hparams))\n    return np.maximum(1e-10, np.dot(_inv_mel_basis, mel_spectrogram))\n\ndef _build_mel_basis(hparams):\n    assert hparams['fmax'] <= hparams['sample_rate'] // 2\n    return librosa.filters.mel(sr=hparams['sample_rate'], n_fft=hparams['n_fft'], n_mels=hparams['num_mels'],\n                               fmin=hparams['fmin'], fmax=hparams['fmax'])\n\ndef _amp_to_db(x, hparams):\n    min_level = np.exp(hparams['min_level_db'] / 20 * np.log(10))\n    return 20 * np.log10(np.maximum(min_level, x))\n\ndef _db_to_amp(x):\n    return np.power(10.0, (x) * 0.05)\n\ndef _normalize(S, hparams):\n    if hparams['allow_clipping_in_normalization']:\n        if hparams['symmetric_mels']:\n            return np.clip((2 * hparams['max_abs_value']) * ((S - hparams['min_level_db']) / (-hparams['min_level_db'])) - hparams['max_abs_value'],\n                           -hparams['max_abs_value'], hparams['max_abs_value'])\n        else:\n            return np.clip(hparams['max_abs_value'] * ((S - hparams['min_level_db']) / (-hparams['min_level_db'])), 0, hparams['max_abs_value'])\n    \n    assert S.max() <= 0 and S.min() - hparams['min_level_db'] >= 0\n    if hparams['symmetric_mels']:\n        return (2 * hparams['max_abs_value']) * ((S - hparams['min_level_db']) / (-hparams['min_level_db'])) - hparams['max_abs_value']\n    else:\n        return hparams['max_abs_value'] * ((S - hparams['min_level_db']) / (-hparams['min_level_db']))\n\ndef _denormalize(D, hparams):\n    if hparams['allow_clipping_in_normalization']:\n        if hparams['symmetric_mels']:\n            return (((np.clip(D, -hparams['max_abs_value'],\n                              hparams['max_abs_value']) + hparams['max_abs_value']) * -hparams['min_level_db'] / (2 * hparams['max_abs_value']))\n                    + hparams.min_level_db)\n        else:\n            return ((np.clip(D, 0, hparams['max_abs_value']) * -hparams['min_level_db'] / hparams['max_abs_value']) + hparams['min_level_db'])\n    \n    if hparams['symmetric_mels']:\n        return (((D + hparams['max_abs_value']) * -hparams['min_level_db'] / (2 * hparams['max_abs_value'])) + hparams.min_level_db)\n    else:\n        return ((D * -hparams['min_level_db'] / hparams['max_abs_value']) + hparams['min_level_db'])\n\n\ndef db_from_amp(x):\n    return 20. * np.log10(np.maximum(1e-5, x))\n\ndef amp_from_db(x):\n    return np.power(10., x / 20.)\n\ndef angle(z):\n    return np.arctan2(np.imag(z), np.real(z))\n\ndef cast_complex(x): \n    complex_x = x.astype(np.complex64)\n    return complex_x\n\ndef make_complex(mag, phase):\n    mag = cast_complex(mag)\n    phase = cast_complex(phase)\n    compex_arr = mag * (np.cos(phase) + 1j*np.sin(phase))\n    return compex_arr\n\ndef normalize_mag(x, min_val=-100, max_val=80):\n    return (x - min_val)/float(max_val - min_val)\n\ndef normalize_phase(x, min_val=-np.pi, max_val=np.pi):\n    return (x - min_val)/float(max_val - min_val)\n\ndef unnormalize_mag(y, min_val=-100, max_val=80):\n    return float(max_val - min_val) * y + min_val\n\ndef unnormalize_phase(y, min_val=-np.pi, max_val=np.pi):\n    return float(max_val - min_val) * y + min_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\nimport numpy as np\n# import audio.audio_utils as audio\n# import audio.hparams as hparams\nimport random\nimport os\nimport librosa\n     \nclass DenoisingDataset(Dataset):\n\n    def __init__(self, train_path, sampling_rate):\n\n        self.files = train_path \n#         self.clean_files = get_clean_list(train_path)\n        self.sampling_rate = sampling_rate\n        \n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n\n        while(1):\n            index = random.randint(0, len(self.files) - 1)\n            fname = self.files[index]\n\n            mel, stft, y = self.process_audio(fname)\n\n            if mel is None or stft is None or y is None:\n                continue\n\n            inp_mel = torch.FloatTensor(np.array(mel)).unsqueeze(1)\n            inp_stft = torch.FloatTensor(np.array(stft))\n            gt_stft = torch.FloatTensor(np.array(y))\n\n            return inp_mel, inp_stft, gt_stft \n\n\n    def process_audio(self, file):\n\n        # Load the gt wav file\n        try:\n            mix_wav = load_wav(file, self.sampling_rate)                   # m\n        except:\n            return None, None, None\n\n        # Mix the noisy wav file with the clean GT file\n        try:\n            clean_wav = load_wav(get_clean_file(file), self.sampling_rate)    \n        except:\n            return None, None, None\n        noisy_wav = mix_wav - clean_wav\n\n        # Extract the corresponding audio segments of 1 second\n        start_idx, gt_seg_wav, noisy_seg_wav = self.crop_audio_window(clean_wav, mix_wav, noisy_wav)\n        \n        if start_idx is None or gt_seg_wav is None or noisy_seg_wav is None:\n            return None, None, None\n\n\n        # -----------------------------------STFTs--------------------------------------------- #\n        # Get the STFT, normalize and concatenate the mag and phase of GT and noisy wavs\n        gt_spec = self.get_spec(gt_seg_wav)                                     # Tx514\n\n        noisy_spec = self.get_spec(noisy_seg_wav)                               # Tx514 \n        # ------------------------------------------------------------------------------------- #\n\n\n        # -----------------------------------Melspecs------------------------------------------ #                          \n        noisy_mels = self.get_segmented_mels(start_idx, noisy_wav)              # Tx80x16\n        if noisy_mels is None:\n            return None, None, None\n        # ------------------------------------------------------------------------------------- #\n        \n        # Input to the lipsync student model: Noisy melspectrogram\n        inp_mel = np.array(noisy_mels)                                          # Tx80x16\n\n        # Input to the denoising model: Noisy linear spectrogram\n        inp_stft = np.array(noisy_spec)                                         # Tx514\n\n        # GT to the denoising model: Clean linear spectrogram\n        gt_stft = np.array(gt_spec)                                             # Tx514\n\n        \n        return inp_mel, inp_stft, gt_stft\n\n\n    def crop_audio_window(self, gt_wav, noisy_wav, random_wav):\n\n        if gt_wav.shape[0] - hparams['wav_step_size'] <= 1280: \n            return None, None, None\n\n        # Get 1 second random segment from the wav\n        start_idx = np.random.randint(low=1280, high=gt_wav.shape[0] - hparams['wav_step_size'])\n        end_idx = start_idx + hparams['wav_step_size']\n        gt_seg_wav = gt_wav[start_idx : end_idx]\n        \n        if len(gt_seg_wav) != hparams['wav_step_size']: \n            return None, None, None\n\n        noisy_seg_wav = noisy_wav[start_idx : end_idx]\n        if len(noisy_seg_wav) != hparams['wav_step_size']: \n            return None, None, None\n\n        # Data augmentation\n        aug_steps = np.random.randint(low=0, high=3200)\n        aug_start_idx = np.random.randint(low=0, high=hparams['wav_step_size']- aug_steps)\n        aug_end_idx = aug_start_idx+aug_steps\n\n        aug_types = ['zero_speech', 'reduce_speech', 'increase_noise']\n        aug = random.choice(aug_types)\n\n        if aug == 'zero_speech':    \n            noisy_seg_wav[aug_start_idx:aug_end_idx] = 0.0\n            \n        elif aug == 'reduce_speech':\n            noisy_seg_wav[aug_start_idx:aug_end_idx] = 0.1*gt_seg_wav[aug_start_idx:aug_end_idx]\n\n        elif aug == 'increase_noise':\n            random_seg_wav = random_wav[start_idx : end_idx]\n            noisy_seg_wav[aug_start_idx:aug_end_idx] = gt_seg_wav[aug_start_idx:aug_end_idx] + (2*random_seg_wav[aug_start_idx:aug_end_idx])\n\n        return start_idx, gt_seg_wav, noisy_seg_wav\n\n\n    def crop_mels(self, start_idx, noisy_wav):\n        \n        end_idx = start_idx + 3200\n\n        # Get the segmented wav (0.2 second)\n        noisy_seg_wav = noisy_wav[start_idx : end_idx]\n        if len(noisy_seg_wav) != 3200: \n            return None\n        \n        # Compute the melspectrogram using librosa\n        spec = melspectrogram(noisy_seg_wav, hparams).T              # 16x80\n        spec = spec[:-1] \n\n        return spec\n\n\n    def get_segmented_mels(self, start_idx, noisy_wav):\n\n        mels = []\n        if start_idx - 1280 < 0: \n            return None\n\n        # Get the overlapping continuous segments of noisy mels\n        for i in range(start_idx, start_idx + hparams['wav_step_size'],40): \n            m = self.crop_mels(i - 1280, noisy_wav)                             # Hard-coded to get 0.2sec segments (5 frames)\n            if m is None or m.shape[0] != hparams['mel_step_size']:\n                return None\n            mels.append(m.T)\n\n        mels = np.asarray(mels)                                             # Tx80x16\n\n        return mels\n\n\n    def get_spec(self, wav):\n\n        # Compute STFT using librosa\n        stft = librosa.stft(y=wav, n_fft=hparams['n_fft_den'], \\\n               hop_length=hparams['hop_size_den'], win_length=hparams['win_size_den']).T\n        stft = stft[:-1]                                                        # Tx257\n\n        # Decompose into magnitude and phase representations\n        mag = np.abs(stft)\n        mag = db_from_amp(mag)\n        phase = angle(stft)\n\n        # Normalize the magnitude and phase representations\n        norm_mag = normalize_mag(mag)\n        norm_phase = normalize_phase(phase)\n            \n        # Concatenate the magnitude and phase representations\n        spec = np.concatenate((norm_mag, norm_phase), axis=1)               # Tx514\n        \n        return spec\n\n\ndef load_data( train_path, num_workers, batch_size=4, sampling_rate=16000, shuffle=False):\n    \n    dataset = DenoisingDataset( train_path, sampling_rate)\n\n    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n\n    return data_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataloader=load_data(train_noisy_paths, 4, batch_size=4, sampling_rate=16000, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx, batch in enumerate(dataloader):\n#     print(f'{idx}: {batch[1].size()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\nimport numpy as np\n\nclass MyModel2D(nn.Module):\n    def __init__(self):\n        super(MyModel2D, self).__init__()\n        self.pad = nn.ZeroPad2d((0, 0, 3, 4))\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=(9, 16), stride=(1, 1), padding=(0, 0), bias=False)\n        self.relu = nn.PReLU()\n        self.batchnorm1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm3 = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm5 = nn.BatchNorm2d(256)\n        \n        self.conv5_1 = nn.Conv2d(256, 512, kernel_size=(3, 1), stride=(8, 1), padding=(1, 0), bias=False)\n        self.batchnorm5_1 = nn.BatchNorm2d(512)\n        self.conv6_1 = nn.ConvTranspose2d(512, 256, kernel_size=(3, 1), stride=(8, 1), padding=(1, 0), bias=False)\n        self.batchnorm6_1 = nn.BatchNorm2d(256)\n        \n        self.conv6 = nn.ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm6 = nn.BatchNorm2d(128)\n        self.conv7 = nn.ConvTranspose2d(128, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm7 = nn.BatchNorm2d(64)\n        self.conv8 = nn.ConvTranspose2d(64, 32, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm8 = nn.BatchNorm2d(32)\n        self.conv9 = nn.ConvTranspose2d(32, 16, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm9 = nn.BatchNorm2d(16)\n        self.spatialdropout = nn.Dropout2d(0.2)\n        self.conv10 = nn.Conv2d(16, 1, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n\n        self.pad2 = nn.ZeroPad2d((0, 0, 2, 1))\n        self.pad3 = nn.ZeroPad2d((0, 0, 1, 2))\n        self.pad4 = nn.ZeroPad2d((0, 0, 0, 1))\n    def forward(self, x):\n        #print(x.size())\n        x = self.pad(x)\n        #print(x.size())\n        #x = self.conv1(x)\n        skip9 = self.conv1(x)\n        x = self.relu(skip9)\n        #print(x.size())\n        x = self.batchnorm1(x)\n        skip8 = self.conv2(x)\n        x = self.relu(skip8)\n        #print(x.size())\n        x = self.batchnorm2(x)\n        skip7 = self.conv3(x)\n        x = self.relu(skip7)\n        #print(x.size())\n        x = self.batchnorm3(x)\n        skip6 = self.conv4(x)\n        x = self.relu(skip6)\n        #print(x.size())\n        x = self.batchnorm4(x)\n        skip6_1 = self.conv5(x)\n        #x = self.pad3(x)\n        #print(x.size())\n        x = self.relu(skip6_1)\n        #print(x.size())\n        #x = self.batchnorm5(x)\n        \n        x = self.conv5_1(x)\n        #x = self.pad3(x)\n        #print(x.size())\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm5_1(x)\n        #x = self.upsample2(x)\n        x = self.conv6_1(x)\n        #print(x.size())\n        x = x + skip6_1\n        \n        \n        \n        #x = self.upsample(x)\n        x = self.conv6(x)\n        #print(x.size())\n        #x = x + skip6\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm6(x)\n        #x = self.upsample(x)\n        #x = self.pad4(x)\n        x = self.conv7(x)\n        x = self.pad2(x)\n        x = x + skip7\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm7(x)\n        #x = self.upsample(x)\n        x = self.conv8(x)\n        #x = self.pad3(x)\n        #x = x + skip8\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm8(x)\n        #x = self.upsample(x)\n        #x = self.pad4(x)\n        x = self.conv9(x)\n        x = self.pad3(x)\n        x = x + skip9\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm9(x)\n        x = self.spatialdropout(x)\n        x = self.conv10(x)\n        \n        x = self.pad4(x)\n        #print(x.size())\n        return x\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class encoder(nn.Module):\n    def __init__(self):\n        super(encoder, self).__init__()\n        self.conv1_1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.norm1 = nn.BatchNorm2d(16)\n        self.act = nn.LeakyReLU()\n        self.conv1_2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2_1 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n        self.norm2 = nn.BatchNorm2d(16)\n        self.conv2_2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n        self.conv3_1 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.norm3 = nn.BatchNorm2d(32)\n        self.conv3_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        self.conv4_1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        self.norm4 = nn.BatchNorm2d(32)\n        self.conv4_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        self.conv5_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.norm5 = nn.BatchNorm2d(64)\n        self.conv5_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.conv6_1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.norm6 = nn.BatchNorm2d(64)\n        self.conv6_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.conv7_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.norm7 = nn.BatchNorm2d(128)\n        self.conv7_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.conv8_1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.norm8 = nn.BatchNorm2d(128)\n        self.conv8_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.conv9_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.norm9 = nn.BatchNorm2d(256)\n        self.conv9_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        \n        self.conv0_1 = nn.Conv2d(256, 256, kernel_size=1, stride=2, padding=0)\n        self.norm9 = nn.BatchNorm2d(256)\n        self.conv0_2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n        self.up = nn.Upsample(scale_factor=2, mode='bicubic')\n        \n        self.deconv1_0 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1,padding=1)\n        self.deconv1_1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.deconv1_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.deconv2_0 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1,padding=1)\n        self.deconv2_1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.deconv2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.deconv3_0 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1,padding=1)\n        self.deconv3_1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        self.deconv3_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        self.deconv4_0 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1,padding=1)\n        self.deconv4_1 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n        self.deconv4_2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n        self.deconv5_0 = nn.ConvTranspose2d(16, 4, kernel_size=3, stride=1,padding=1)\n        self.deconv5_1 = nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n        self.deconv5_2 = nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n        self.deconv6 = nn.Conv2d(4, 1, kernel_size=1)\n        \n    def forward(self, audio_sequences):\n        print('Start!!!') \n        B = audio_sequences.size()\n#         print(B)\n        audio_sequences = torch.unsqueeze(audio_sequences, 1)\n        input_dim_size = len(audio_sequences.size())\n#         if input_dim_size < 3:\n#             audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n        print(audio_sequences.size())\n        x = self.conv1_1(audio_sequences) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv1_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv2_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv2_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv3_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv3_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv4_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv4_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv5_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv5_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv6_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv6_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv7_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv7_2(x)\n        x = self.act(x)\n        print(x.size())\n        x = self.conv8_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.conv8_2(x)\n        x = self.act(x)\n        print(x.size())\n        x = self.conv9_1(x) \n        print(x.size())\n        x = self.act(x)\n        x = self.conv9_2(x) \n        print(x.size())\n        x = self.act(x)\n        print('Encoder finish')\n        \n        x = self.up(x) \n        print(x.size())\n        x = self.conv0_1(x)\n        print(x.size())\n        x = self.act(x)\n        x = self.conv0_2(x)\n        print(x.size())\n        x = self.act(x)\n        x = self.up(x) \n        print(x.size())\n        x = self.conv0_1(x)\n        print(x.size())\n        x = self.act(x)\n        x = self.conv0_2(x)\n        print(x.size())\n        x = self.act(x)\n        x = self.up(x) \n        print(x.size())\n        x = self.conv0_1(x)\n        print(x.size())\n        x = self.act(x)\n        x = self.conv0_2(x)\n        print(x.size())\n        x = self.act(x)\n        x = self.up(x) \n        print(x.size())\n        x = self.conv0_1(x)\n        print(x.size())\n        x = self.act(x)\n        x = self.conv0_2(x)\n        print(x.size())\n        x = self.act(x)\n        print('Up finish')\n\n        \n        x = self.deconv1_0(x) \n        print(x.size())\n        x = self.deconv1_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv1_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv2_0(x) \n        print(x.size())\n        x = self.deconv2_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv2_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv3_0(x) \n        print(x.size())\n        x = self.deconv3_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv3_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv4_0(x) \n        print(x.size())\n        x = self.deconv4_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv4_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv5_0(x) \n        print(x.size())\n        x = self.deconv5_1(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv5_2(x) \n        x = self.act(x)\n        print(x.size())\n        x = self.deconv6(x) \n        print(x.size())\n        print('Decoder finish')\n\n            \n        return self.act(x)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport argparse\n# from models import *\n# import audio.hparams as hparams \n# from scripts.data_loader import *\nfrom tqdm import tqdm\nimport librosa\nimport torch\nimport torch.optim as optim\nimport cv2\nimport subprocess\n\n# Initialize the global variables\nuse_cuda = torch.cuda.is_available()\nprint('use_cuda: {}'.format(use_cuda))\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n\n# Function to reconstruct the wav from the magnitude and phase representations\ndef reconstruct_wav(stft):\n\n    mag = stft[:257, :]\n    phase = stft[257:, :]\n    \n    denorm_mag = unnormalize_mag(mag)\n    denorm_phase = unnormalize_phase(phase)\n    recon_mag = amp_from_db(denorm_mag)\n    complex_arr = make_complex(recon_mag, denorm_phase)\n    print(complex_arr.shape)\n    wav = librosa.istft(complex_arr, hop_length=hparams['hop_size_den'], win_length=hparams['win_size_den'])\n    \n    return wav\n \n# Function to generate and save the sample audio/video files\ndef save_samples(gt_stft, inp_stft, output_stft, faces, epoch, checkpoint_dir):\n\n    gt_stft = gt_stft.detach().cpu().numpy()\n    inp_stft = inp_stft.detach().cpu().numpy()\n    output_stft = output_stft.detach().cpu().numpy()\n    faces = faces.permute(0,2,3,4,1)\n    faces = faces.detach().cpu().numpy()\n\n    folder = join(checkpoint_dir, \"samples_step{:04d}\".format(epoch))\n    if not os.path.exists(folder): \n        os.mkdir(folder)\n\n    for step in range((gt_stft.shape[0])): \n\n        # Save GT audio\n        gt = gt_stft[step]\n        gt_wav = reconstruct_wav(gt.T)\n        gt_aud_fname = os.path.join(folder, str(step)+'_gt.wav')\n        librosa.output.write_wav(gt_aud_fname, gt_wav, 16000) \n\n        # Save input audio\n        inp = inp_stft[step]\n        inp_wav = reconstruct_wav(inp.T)\n        inp_aud_fname = os.path.join(folder, str(step)+'_inp.wav')\n        librosa.output.write_wav(inp_aud_fname, inp_wav, 16000)            \n        \n        # Save generated audio\n        generated = output_stft[step]\n        generated_wav = reconstruct_wav(generated.T)\n        generated_aud_fname = os.path.join(folder, str(step)+'_pred.wav')\n        librosa.output.write_wav(generated_aud_fname, generated_wav, 16000)            \n\n        # Save generated video\n        generated_vid_fname = os.path.join(folder, str(step)+'_pred')\n        generate_video(faces[step], generated_aud_fname, generated_vid_fname)     \n    \n    print(\"Saved samples:\", folder)\n\n\ndef train(device, model, train_loader, test_loader, optimizer, epoch_resume, total_epochs, checkpoint_dir, args):\n\n    l1_loss = nn.MSELoss()\n\n    for epoch in range(epoch_resume+1, total_epochs+1):\n\n        print(\"Epoch %d\" %epoch)\n\n        total_loss = 0.0\n#         for idx, batch in enumerate(train_loader):\n#             print(f'{idx}: {batch[0]}')\n            \n        progress_bar = tqdm(train_loader)\n        step = 0\n        \n        \n\n        for (inp_mel, inp_stft, gt_stft) in progress_bar:\n            print('Loder запущен')\n            model.train()\n            optimizer.zero_grad()\n            \n\n            # Transform data to CUDA device\n            inp_mel = inp_mel.to(device)                                        # BxTx1x80x16\n            inp_stft = inp_stft.to(device)                                      # BxTx514\n            gt_stft = gt_stft.to(device)                                        # BxTx514\n            \n            \n            # Generate the clean stft\n            output_stft = model(inp_stft)                               # BxTx514\n\n            # Compute the L1 reconstruction loss\n            loss = l1_loss(output_stft, gt_stft)\n            total_loss += loss.item()\n            \n            # Backpropagate\n            loss.backward()\n            optimizer.step()\n\n            # Display the training progress\n            progress_bar.set_description('Loss: {}'.format(total_loss / (step + 1))) \n            inp_stft = np.transpose(inp_stft.cpu().detach().numpy()[0])\n            gt_stft = np.transpose(gt_stft.cpu().detach().numpy()[0])\n            output_stft = np.transpose(output_stft.cpu().detach().numpy()[0])\n            progress_bar.refresh()\n            step+=1\n        if epoch > 1:\n            return reconstruct_wav(inp_stft) , reconstruct_wav(output_stft), reconstruct_wav(gt_stft) \n\n        train_loss = total_loss / total_batch\n        \n\n        # Save the checkpoint\n        '''if epoch % args.ckpt_freq == 0:\n\n            # Save the model\n            save_checkpoint(model, optimizer, train_loss, checkpoint_dir, epoch)'''\n\n        # Validation loop\n        '''if epoch % args.validation_interval == 0:\n            with torch.no_grad():\n                validate(device, model, test_loader, epoch, checkpoint_dir)'''\n\n    \ndef validate(device, model, test_loader, epoch, checkpoint_dir):\n\n    print('\\nEvaluating for {} steps'.format(len(test_loader)))\n\n    l1_loss = nn.L1Loss()\n\n    losses = []\n\n    for step, (inp_mel, inp_stft, gt_stft) in enumerate(test_loader):\n\n        model.eval()\n\n        # Transform data to CUDA device\n        inp_mel = inp_mel.to(device)\n        inp_stft = inp_stft.to(device)\n        gt_stft = gt_stft.to(device)\n        \n\n        # Generate the clean stft\n        output_stft = model(inp_stft)\n\n        # Compute the L1 reconstruction loss\n        loss = l1_loss(output_stft, gt_stft)\n        losses.append(loss.item())\n\n    # Compute the average of the validation loss\n    averaged_loss = sum(losses) / len(losses)\n    print(\"Validation loss: \", averaged_loss)\n\n    # Save the GT and the denoised files\n    save_samples(gt_stft, inp_stft, output_stft, faces, epoch, checkpoint_dir)\n\n    return\n\ndef save_checkpoint(model, optimizer, train_loss, checkpoint_dir, epoch):\n\n    checkpoint_path = join(checkpoint_dir, \"checkpoint_step{:04d}.pt\".format(epoch))\n\n    torch.save({\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"loss\": train_loss,\n        \"epoch\": epoch,\n    }, checkpoint_path)\n    \n    print(\"Saved checkpoint:\", checkpoint_path)\n\ndef _load(checkpoint_path):\n    \n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n\n    return checkpoint\n\n\ndef load_checkpoint(path, model, optimizer, reset_optimizer=False):\n\n    print(\"Load checkpoint from: {}\".format(path))\n    checkpoint = _load(path)\n    s = checkpoint[\"state_dict\"]\n    new_s = {}\n\n    for k, v in s.items():\n        if hparams['n_gpu'] > 1:\n            if not k.startswith('module.'):\n                new_s['module.'+k] = v\n            else:\n                new_s[k] = v\n        else:\n            new_s[k.replace('module.', '')] = v\n\n    model.load_state_dict(new_s)\n\n    epoch_resume = 0\n    if not reset_optimizer:\n        optimizer_state = checkpoint[\"optimizer\"]\n        if optimizer_state is not None:\n            print(\"Load optimizer state from {}\".format(path))\n            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n        epoch_resume = checkpoint['epoch']\n        loss = checkpoint['loss']\n\n        print(\"Model resumed for training...\")\n        print(\"Epoch: \", epoch_resume)\n        print(\"Loss: \", loss)\n    \n    return model, epoch_resume\n\n# Call the data generator to get the data\ntrain_loader = load_data(train_path=train_noisy_paths,\n                         num_workers= 4,                 # hparams['num_workers'], \n                         batch_size= 14,                   # hparams['batch_size'],\n                         shuffle=True)\n\ntotal_batch = len(train_loader)\nprint(\"Total train batch: \", total_batch)\n\ntest_loader = load_data(train_path= tast_noisy_paths, num_workers=hparams['num_workers'], batch_size=hparams['batch_size'], shuffle=False)\n\n# Initialize the Denoising model \nmodel = MyModel2D()\n# model = encoder()\nif hparams['n_gpu'] > 1:\n    print(\"Using\", hparams['n_gpu'], \"GPUs for the denoising model!\")\n    model = nn.DataParallel(model)\nelse:\n    print(\"Using single GPU for the denoising model!\")\nmodel.to(device)\n    \nprint('Total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n\n    # Set the learning rate\nif hparams['reduced_learning_rate'] is not None:\n    lr = hparams['reduced_learning_rate']\nelse:\n    lr = hparams['initial_learning_rate']\n\n# Set the optimizer\noptimizer = optim.Adam(list(model.parameters()), lr=lr) # [p for p in model.parameters() if p.requires_grad]\n\n# Resume the denoising model for training if the path is provided\nargs = dict(checkpoint_path = None,\n            continue_epoch = True,\n           checkpoint_dir = '/kaggle/working' )\nepoch_resume=0\nif args['checkpoint_path'] is not None:\n    model, epoch_resume = load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n\nif args['continue_epoch']==True:\n    epoch = epoch_resume\nelse:\n    epoch = 0\n\n# Create the folder to save checkpoints\ncheckpoint_dir = args['checkpoint_dir']\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Train!\ntr = train(device, model, train_loader, test_loader, optimizer, epoch, hparams['nepochs'], checkpoint_dir, args)\nIPython.display.Audio(data=tr[0], rate=16000)\nIPython.display.Audio(data=tr[1], rate=16000)\nprint(\"Finished\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Audio(data=tr[0], rate=16000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Audio(data=tr[1], rate=16000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Audio(data=tr[2], rate=16000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nfrom IPython.display import clear_output\nfrom PIL import Image\nfrom matplotlib import cm\nfrom time import perf_counter\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ntrain_loader = load_data(train_path=train_noisy_paths, num_workers=hparams['num_workers'], batch_size=hparams['batch_size'], shuffle=True)\n\ndef train(model: nn.Module) -> float:\n    model.train()\n\n    train_loss = 0\n\n    for x, y in tqdm(train_loader, desc='Train'):\n        optimizer.zero_grad()\n\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        train_loss += loss.item()\n\n        loss.backward()\n\n        optimizer.step()\n\n    train_loss /= len(train_loader)\n    \n    return train_loss\n\n@torch.inference_mode()\ndef evaluate(model: nn.Module, loader: DataLoader):\n    model.eval()\n\n    total_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(loader, desc='Evaluation'):\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        total_loss += loss.item()\n\n        _, y_pred = torch.max(output, 1)\n        total += y.size(0)\n        correct += (y_pred == y).sum().item()\n\n    total_loss /= len(loader)\n    accuracy = correct / total\n\n    return total_loss, accuracy\n\ndef plot_stats(\n    train_loss,\n    valid_loss,\n    valid_accuracy,\n    title: str\n):\n    plt.figure(figsize=(16, 8))\n\n    plt.title(title + ' loss')\n\n    plt.plot(train_loss, label='Train loss')\n    plt.plot(valid_loss, label='Valid loss')\n    plt.legend()\n    plt.grid()\n\n    plt.show()\n\n    plt.figure(figsize=(16, 8))\n\n    plt.title(title + ' accuracy')\n\n    plt.plot(valid_accuracy)\n    plt.grid()\n\n    plt.show()\n\nmodel = UNetLikeModel()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nloss_fn = nn.L1Loss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 15\n\ntrain_loss_history, valid_loss_history = [], []\nvalid_accuracy_history = []\n\nstart = perf_counter()\naccuracy = 0\nvalid_loader = test_loader\n\n\nfor epochs in range(num_epochs):\n    train_loss = train(model)\n\n    valid_loss, valid_accuracy = evaluate(model, valid_loader)\n\n    train_loss_history.append(train_loss)\n    valid_loss_history.append(valid_loss)\n    valid_accuracy_history.append(valid_accuracy)\n    plot_stats(train_loss_history, valid_loss_history, valid_accuracy_history, 'MLP model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}